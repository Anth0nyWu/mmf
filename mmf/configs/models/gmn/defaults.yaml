model_config:
  gmn: &gmn
    model_data_dir: ${env.data_dir}

    # text_embeddings:
    # - type: attention
    #   params:
    #     hidden_dim: 1024
    #     num_layers: 1
    #     conv1_out: 512
    #     conv2_out: 2
    #     dropout: 0
    #     embedding_dim: 300 
    #     kernel_size: 1
    #     padding: 0
    #     model_data_dir: ${model_config.gmn.model_data_dir}
    text_embeddings:
    - type: bilstm
      params:
        hidden_dim: 2048
        # ??? out put size = hidden_dim
        out_dim: 2048
        # vocab_size: 1280
        embedding_dim: 300
        num_layers: 1
        dropout: 0.0
        bidirectional: False
        rnn_type: 'GRU'
        model_data_dir: ${model_config.gmn.model_data_dir}
    
    image_feature_embeddings:
    - modal_combine:
        type: non_linear_element_multiply
        params:
          dropout: 0
          hidden_dim: 5000

      normalization: softmax
      transform:
        type: linear
        params:
          out_dim: 1
    image_feature_dim: 2048 # 2048

    image_feature_encodings:
    - type: finetune_faster_rcnn_fpn_fc7
      params:
        bias_file: models/detectron.defaults/fc7_b.pkl
        weights_file: models/detectron.defaults/fc7_w.pkl
        model_data_dir: ${model_config.gmn.model_data_dir}
    - type: default
      params:
        model_data_dir: ${model_config.gmn.model_data_dir}

    losses:
    - type: logit_bce
    
    classifier:
      type: logit
      params:
        img_hidden_dim: 5000
        text_hidden_dim: 300

    image_text_modal_combine:
      type: non_linear_element_multiply
      params:
        dropout: 0
        hidden_dim: 5000

    mem_nn:
      max_story_len: 196
      embd_size: 70
      vocab_size: 2048 # fix 2048
      ans_size: 300
    mcb:
      visual_input_dim: 2048 # 2048
      textual_input_dim: 2048 # 2048
      output_dim: 2048 # 1024
